{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import shap\n",
    "import lime.lime_tabular\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, target_variable='churn', cap_limit=0.95):\n",
    "    irrelevant_cols = ['clientnum', 'customerid', 'surname', 'visitorid', 'id']\n",
    "    df = df.drop([col for col in irrelevant_cols if col in df.columns], axis=1, errors='ignore')\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if df[col].dtype in ['float64', 'int64']:\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "        else:\n",
    "            df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "    \n",
    "    for col in df.select_dtypes(include=['float64', 'int64']).columns:\n",
    "        upper_limit = df[col].quantile(cap_limit)\n",
    "        df[col] = df[col].clip(upper=upper_limit)\n",
    "    \n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    df[categorical_cols] = df[categorical_cols].astype(str)\n",
    "    \n",
    "    X = df.drop(target_variable, axis=1)\n",
    "    y = df[target_variable]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def perform_eda(df):\n",
    "    print(\"Dataset Information:\")\n",
    "    print(df.info())\n",
    "    print(\"\\nMissing Values:\")\n",
    "    print(df.isnull().sum())\n",
    "    print(\"\\nDescriptive Statistics:\")\n",
    "    print(df.describe(include='all'))\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.countplot(x='churn', data=df)\n",
    "    plt.title('Distribution of Target Variable')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "    plt.title('Correlation Heatmap')\n",
    "    plt.show()\n",
    "\n",
    "def feature_engineering(X):\n",
    "    numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "    if not numeric_cols.empty:\n",
    "        X[numeric_cols] = (X[numeric_cols] - X[numeric_cols].mean()) / X[numeric_cols].std()\n",
    "    \n",
    "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "    if not categorical_cols.empty:\n",
    "        X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
    "    \n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pipeline(X):\n",
    "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "    numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "    \n",
    "    numerical_pipeline = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_pipeline = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('num', numerical_pipeline, numerical_cols),\n",
    "        ('cat', categorical_pipeline, categorical_cols)\n",
    "    ])\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "def split_data(X, y):\n",
    "    return train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "def train_models(X_train, X_test, y_train, y_test):\n",
    "    models = {\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "        'XGBoost': xgb.XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "    }\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(f\"{model_name} Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "        print(f\"{model_name} Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "        print(f\"{model_name} Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "def global_explanations(model, X):\n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer(X)\n",
    "    shap.summary_plot(shap_values, X)\n",
    "\n",
    "def local_explanations(model, X, sample_index=0):\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "        training_data=X.values,\n",
    "        feature_names=X.columns,\n",
    "        class_names=['not_churn', 'churn'],\n",
    "        mode='classification'\n",
    "    )\n",
    "    explanation = explainer.explain_instance(X.iloc[sample_index], model.predict_proba)\n",
    "    explanation.show_in_notebook()\n",
    "\n",
    "def surrogate_model(model, X):\n",
    "    dt = DecisionTreeClassifier()\n",
    "    dt.fit(X, y)\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    _ = plot_tree(dt, feature_names=X.columns, filled=True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def churn_prediction_pipeline(file_path, target_variable='churn'):\n",
    "    df = pd.read_csv(file_path)\n",
    "    perform_eda(df)\n",
    "    X, y = preprocess_data(df, target_variable)\n",
    "    X = feature_engineering(X)\n",
    "    preprocessor = build_pipeline(X)\n",
    "    X = preprocessor.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = split_data(X, y)\n",
    "    train_models(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model_dataset1.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFeatures\u001b[39;00m(BaseModel):\n\u001b[0;32m      4\u001b[0m     features: \u001b[38;5;28mdict\u001b[39m\n\u001b[0;32m      6\u001b[0m models \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset1\u001b[39m\u001b[38;5;124m'\u001b[39m: joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_dataset1.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset2\u001b[39m\u001b[38;5;124m'\u001b[39m: joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_dataset2.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m      9\u001b[0m }\n\u001b[0;32m     11\u001b[0m \u001b[38;5;129m@app\u001b[39m\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/predict/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(dataset_name: \u001b[38;5;28mstr\u001b[39m, features: Features):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dataset_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m models:\n",
      "File \u001b[1;32md:\\Conda\\Lib\\site-packages\\joblib\\numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    648\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    651\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[0;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model_dataset1.pkl'"
     ]
    }
   ],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "class Features(BaseModel):\n",
    "    features: dict\n",
    "\n",
    "models = {\n",
    "    'dataset1': joblib.load('model_dataset1.pkl'),\n",
    "    'dataset2': joblib.load('model_dataset2.pkl'),\n",
    "}\n",
    "\n",
    "@app.post(\"/predict/\")\n",
    "def predict(dataset_name: str, features: Features):\n",
    "    if dataset_name not in models:\n",
    "        raise HTTPException(status_code=404, detail=\"Dataset not found\")\n",
    "    \n",
    "    model = models[dataset_name]\n",
    "    df = pd.DataFrame([features.features])\n",
    "    X = feature_engineering(df)\n",
    "    X_transformed = preprocessor.transform(X)\n",
    "    prediction = model.predict(X_transformed)\n",
    "    \n",
    "    return {\"prediction\": prediction[0]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
